{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":12993472,"sourceType":"competition"},{"sourceId":12569884,"sourceType":"datasetVersion","datasetId":1346}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport warnings\n\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\nfrom scipy.stats import rankdata, pearsonr\n\nimport shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:05:33.239452Z","iopub.execute_input":"2025-07-25T02:05:33.239815Z","iopub.status.idle":"2025-07-25T02:05:41.378383Z","shell.execute_reply.started":"2025-07-25T02:05:33.239787Z","shell.execute_reply":"2025-07-25T02:05:41.377833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(dataframe, dataset):\n    \"\"\"\n    Reduces the memory footprint of a DataFrame by downcasting numeric columns \n    to the most efficient data types that can safely hold the data.\n\n    This function iterates over each column in the DataFrame and attempts to:\n    - Downcast integer columns to the smallest possible int subtype (int8, int16, etc.).\n    - Downcast float columns to the smallest possible float subtype (float16, float32, etc.).\n    - Skip non-numeric columns.\n\n    It prints the memory usage before and after optimization, along with the percentage reduction.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame to optimize.\n        dataset (str): A label or name of the dataset for logging purposes.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame with reduced memory usage.\n    \"\"\"\n    print('Reducing memory usage for:', dataset)\n    initial_mem_usage = dataframe.memory_usage().sum() / 1024**2\n    \n    for col in dataframe.columns:\n        col_type = dataframe[col].dtype\n\n        c_min = dataframe[col].min()\n        c_max = dataframe[col].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                dataframe[col] = dataframe[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                dataframe[col] = dataframe[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                dataframe[col] = dataframe[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                dataframe[col] = dataframe[col].astype(np.int64)\n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                dataframe[col] = dataframe[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                dataframe[col] = dataframe[col].astype(np.float32)\n            else:\n                dataframe[col] = dataframe[col].astype(np.float64)\n\n    final_mem_usage = dataframe.memory_usage().sum() / 1024**2\n    print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n    print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n    print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:05:41.379342Z","iopub.execute_input":"2025-07-25T02:05:41.379692Z","iopub.status.idle":"2025-07-25T02:05:41.387896Z","shell.execute_reply.started":"2025-07-25T02:05:41.379664Z","shell.execute_reply":"2025-07-25T02:05:41.387225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_features(df):\n    df = df.copy()\n    df['bid_ask_spread'] = df['ask_qty'] - df['bid_qty']\n    df['bid_ask_ratio'] = df['bid_qty'] / (df['ask_qty'] + 1e-8)\n    df['total_liquidity'] = df['bid_qty'] + df['ask_qty']\n    df['liquidity_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['total_liquidity'] + 1e-8)\n    df['normalized_spread'] = df['bid_ask_spread'] / (df['total_liquidity'] + 1e-8)\n    \n    df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-8)\n    df['net_order_flow'] = df['buy_qty'] - df['sell_qty']\n    df['order_flow_imbalance'] = df['net_order_flow'] / (df['buy_qty'] + df['sell_qty'] + 1e-8)\n    df['volume_participation'] = df['volume'] / (df['buy_qty'] + df['sell_qty'] + 1e-8)\n    df['aggressive_ratio'] = (df['buy_qty'] + df['sell_qty']) / (df['volume'] + 1e-8)\n    \n    df['buy_pressure'] = df['buy_qty'] / (df['volume'] + 1e-8)\n    df['sell_pressure'] = df['sell_qty'] / (df['volume'] + 1e-8)\n    df['net_pressure'] = df['buy_pressure'] - df['sell_pressure']\n    df['pressure_ratio'] = df['buy_pressure'] / (df['sell_pressure'] + 1e-8)\n    \n    df['depth_ratio'] = df['total_liquidity'] / (df['volume'] + 1e-8)\n    df['bid_depth_ratio'] = df['bid_qty'] / (df['volume'] + 1e-8)\n    df['ask_depth_ratio'] = df['ask_qty'] / (df['volume'] + 1e-8)\n    df['depth_imbalance'] = (df['bid_depth_ratio'] - df['ask_depth_ratio']) / (df['depth_ratio'] + 1e-8)\n    \n    df['kyle_lambda'] = np.abs(df['net_order_flow']) / (df['volume'] + 1e-8)\n    df['amihud_illiquidity'] = np.abs(df['net_pressure']) / (df['volume'] + 1e-8)\n    df['liquidity_consumption'] = df['volume'] / (df['total_liquidity'] + 1e-8)\n    \n    df['price_efficiency'] = 1 / (1 + df['amihud_illiquidity'])\n    df['execution_quality'] = df['volume'] / (df['bid_ask_spread'] + 1)\n    \n    df['pin_proxy'] = np.abs(df['order_flow_imbalance']) * df['amihud_illiquidity']\n    df['order_toxicity'] = np.abs(df['order_flow_imbalance']) * df['kyle_lambda']\n    \n    df['bid_momentum'] = df['bid_qty'] * df['buy_qty'] / (df['volume'] + 1e-8)\n    df['ask_momentum'] = df['ask_qty'] * df['sell_qty'] / (df['volume'] + 1e-8)\n    df['liquidity_adjusted_volume'] = df['volume'] / np.sqrt(df['total_liquidity'] + 1)\n    \n    df['log_volume'] = np.log1p(df['volume'])\n    df['log_liquidity'] = np.log1p(df['total_liquidity'])\n    df['log_spread'] = np.log1p(np.abs(df['bid_ask_spread']))\n\n    # Replace any NaNs or Infs\n    df = df.replace([np.inf, -np.inf], 0).fillna(0)\n    \n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:05:41.388651Z","iopub.execute_input":"2025-07-25T02:05:41.388875Z","iopub.status.idle":"2025-07-25T02:05:41.422067Z","shell.execute_reply.started":"2025-07-25T02:05:41.388859Z","shell.execute_reply":"2025-07-25T02:05:41.421571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_statistical_features(df):\n    \"\"\"\n    Adds statistical aggregation features across all 'X'-prefixed columns:\n    - Mean\n    - Std Dev\n    - Range (max - min)\n    - Median\n    - 25th percentile\n    - 75th percentile\n    - Count of values above row mean\n    - Index of max and min column (numeric suffix)\n    - Cleans up NaNs and infs\n    \"\"\"\n    x_cols = [col for col in df.columns if col.startswith('X') and col[1:].isdigit()]\n    x_data = df[x_cols]\n\n    # Core stats\n    df['x_stat_mean'] = x_data.mean(axis=1)\n    df['x_stat_std'] = x_data.std(axis=1)\n    df['x_stat_range'] = x_data.max(axis=1) - x_data.min(axis=1)\n    df['x_stat_median'] = x_data.median(axis=1)\n    df['x_stat_p25'] = x_data.quantile(0.25, axis=1)\n    df['x_stat_p75'] = x_data.quantile(0.75, axis=1)\n\n    # Count of values above row mean\n    row_means = df['x_stat_mean'].values[:, None]\n    df['x_stat_above_mean_count'] = (x_data.values > row_means).sum(axis=1)\n\n    # Index (suffix) of max and min column\n    df['x_stat_idx_max'] = x_data.idxmax(axis=1).str.extract(r'(\\d+)', expand=False).astype(float)\n    df['x_stat_idx_min'] = x_data.idxmin(axis=1).str.extract(r'(\\d+)', expand=False).astype(float)\n\n    # Cleanup: handle infs and NaNs\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(0, inplace=True)\n\n    return df\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:05:51.669020Z","iopub.execute_input":"2025-07-25T02:05:51.669295Z","iopub.status.idle":"2025-07-25T02:05:51.675880Z","shell.execute_reply.started":"2025-07-25T02:05:51.669273Z","shell.execute_reply":"2025-07-25T02:05:51.675016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_non_linear_x_market_interactions(df):\n    \"\"\"\n    Adds non-linear interaction features between 'X'-prefixed columns and selected market features.\n\n    - For each column starting with 'X' followed by digits (e.g., 'X1', 'X23'), creates new features by \n      multiplying the X-column with the logarithm of each available market feature.\n    - Market features considered: volume, buy_qty, sell_qty, x_stat_mean, x_stat_median.\n    - Only uses market features that are present in the DataFrame.\n    - Resulting feature name format: '{X_col}_log_x_{market_feature}'.\n    - Cleans the resulting DataFrame by replacing NaNs and infinities with zero.\n\n    Returns:\n        pd.DataFrame: The input DataFrame with new interaction features added.\n    \"\"\"\n    market_features = ['volume', 'buy_qty', 'sell_qty','x_stat_mean', 'x_stat_median']\n    x_cols = [col for col in df.columns if col.startswith('X') and col[1:].isdigit()]\n    \n    # Filter only available market features\n    market_features = [feat for feat in market_features if feat in df.columns]\n\n    for x_col in x_cols:\n        for m_feat in market_features:\n            new_col = f'{x_col}_log_x_{m_feat}'\n            df[new_col] = df[x_col] * np.log(df[m_feat])\n    \n    # Cleanup\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(0, inplace=True)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:05:51.951847Z","iopub.execute_input":"2025-07-25T02:05:51.952547Z","iopub.status.idle":"2025-07-25T02:05:51.959962Z","shell.execute_reply.started":"2025-07-25T02:05:51.952491Z","shell.execute_reply":"2025-07-25T02:05:51.958996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def select_top_k_features_by_shap(X_train, y_train, feature_names, k=30):\n    \"\"\"Select top-k features using mean absolute SHAP value importance\"\"\"\n    print(\"Calculating SHAP-based feature importance...\")\n\n    # Convert categories to int\n    X_train = X_train.copy()\n    for col in X_train.select_dtypes(include='category').columns:\n        X_train[col] = X_train[col].astype(int)\n\n    # Train XGBoost model\n    params = {\n        'n_estimators': 100,\n        'max_depth': 6,\n        'learning_rate': 0.1,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbosity': 0\n    }\n\n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train)\n\n    # Compute SHAP values\n    explainer = shap.Explainer(model, X_train)\n    shap_values = explainer(X_train)\n\n    # Calculate mean absolute SHAP value per feature\n    mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n    shap_importance_df = pd.DataFrame({\n        'feature': X_train.columns,\n        'importance': mean_abs_shap\n    }).sort_values('importance', ascending=False)\n\n    # Select top-k features\n    selected_features = shap_importance_df.head(k)['feature'].tolist()\n\n    # Always include critical features\n    critical_features = ['order_flow_imbalance', 'kyle_lambda', 'vpin', 'volume',\n                         'bid_ask_spread', 'liquidity_imbalance', 'buying_pressure']\n\n    for feat in critical_features:\n        if feat in feature_names and feat not in selected_features:\n            selected_features.append(feat)\n\n    print(f\"Selected top {k} SHAP features (plus critical ones if needed). Total: {len(selected_features)}\")\n\n    return selected_features, shap_importance_df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:06:38.429611Z","iopub.execute_input":"2025-07-25T02:06:38.430083Z","iopub.status.idle":"2025-07-25T02:06:38.436504Z","shell.execute_reply.started":"2025-07-25T02:06:38.430057Z","shell.execute_reply":"2025-07-25T02:06:38.435695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_time_decay_weights(n, decay=0.95):\n    \"\"\"\n    Generates a sequence of time-decay weights for a series of length `n`.\n\n    Weights are assigned such that more recent observations (towards the end of the series)\n    receive higher weight, following an exponential decay pattern. The weights are normalized\n    to sum to `n`, preserving the original scale.\n\n    Args:\n        n (int): Number of observations.\n        decay (float, optional): Decay rate between 0 and 1. Lower values decay faster.\n                                 Defaults to 0.95.\n\n    Returns:\n        np.ndarray: A 1D array of shape (n,) containing the normalized time-decay weights.\n    \"\"\"\n    pos = np.arange(n)\n    norm = pos / (n - 1)\n    w = decay ** (1.0 - norm)\n    return w * n / w.sum()\n\ndef adjust_weights_for_outliers(X, y, base_weights, outlier_fraction=0.001):\n    \"\"\"\n    Adjusts sample weights to downweight outliers based on model prediction residuals.\n\n    A quick Random Forest model is trained using the provided features and labels\n    to estimate residuals. Samples with the largest residuals (as defined by \n    `outlier_fraction`) are considered outliers and receive lower weights.\n\n    The adjustment scales down the base weights for outliers on a linear scale \n    between 0.2 and 0.8 of the original value, depending on residual severity.\n\n    Args:\n        X (pd.DataFrame or np.ndarray): Feature matrix.\n        y (pd.Series or np.ndarray): Target values.\n        base_weights (np.ndarray): Original sample weights.\n        outlier_fraction (float, optional): Fraction of samples to treat as outliers.\n                                            Defaults to 0.001.\n\n    Returns:\n        np.ndarray: Adjusted sample weights with reduced influence for outliers.\n    \"\"\"\n    # Train quick model to estimate residuals\n    model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n    model.fit(X, y, sample_weight=base_weights)\n    preds = model.predict(X)\n    residuals = np.abs(y - preds)\n\n    # Top N residuals = outliers\n    n_outliers = max(1, int(outlier_fraction * len(residuals)))\n    threshold = np.partition(residuals, -n_outliers)[-n_outliers]\n    outlier_mask = residuals >= threshold\n\n    # Downweight outliers (linear scale: 0.2–0.8 of base weight)\n    adjusted_weights = base_weights.copy()\n    if outlier_mask.any():\n        res_out = residuals[outlier_mask]\n        res_norm = (res_out - res_out.min()) / (res_out.ptp() + 1e-8)\n        weight_factors = 0.8 - 0.6 * res_norm\n        adjusted_weights[outlier_mask] *= weight_factors\n\n    return adjusted_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:06:38.714937Z","iopub.execute_input":"2025-07-25T02:06:38.715175Z","iopub.status.idle":"2025-07-25T02:06:38.721748Z","shell.execute_reply.started":"2025-07-25T02:06:38.715155Z","shell.execute_reply":"2025-07-25T02:06:38.720969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\ntrain = pd.read_parquet(\"/kaggle/input/drw-crypto-market-prediction/train.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/drw-crypto-market-prediction/test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:06:45.130009Z","iopub.execute_input":"2025-07-25T02:06:45.130299Z","iopub.status.idle":"2025-07-25T02:07:35.691683Z","shell.execute_reply.started":"2025-07-25T02:06:45.130278Z","shell.execute_reply":"2025-07-25T02:07:35.691094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_features = list(dict.fromkeys([\n    \"X752\", \"X287\", \"X298\", \"X759\", \"X302\", \"X55\", \"X56\", \"X52\", \"X303\", \"X51\",\n    \"X598\", \"X385\", \"X603\", \"X674\", \"X415\", \"X345\", \"X174\", \"X178\", \"X168\", \"X612\",\n    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\",\n    \"X758\", \"X296\", \"X611\", \"X780\", \"X451\", \"X25\", \"X591\", \"X727\", \"X427\", \"X288\",\n    \"X721\", \"X312\", \"X421\", \"X471\", \"X573\", \"X255\", \"X144\", \"X299\", \"X301\", \"X563\",\n    \"X737\", \"X702\", \"X507\", \"X306\", \"X501\", \"X586\", \"X43\", \"X517\", \"X248\", \"X137\",\n    \"X757\", \"X196\", \"X777\", \"X280\", \"X266\", \"X689\", \"X294\", \"X492\", \"X555\", \"X731\",\n    \"X262\", \"X576\", \"X13\", \"X518\", \"X502\", \"X558\", \"X6\", \"X602\", \"X695\", \"X703\",\n    \"X413\", \"X660\", \"X37\", \"X15\", \"X310\", \"X512\", \"X362\", \"X631\", \"X214\", \"X562\",\n    \"X488\", \"X510\", \"X256\", \"X35\", \"X128\", \"X86\", \"X170\", \"X30\", \"X265\", \"X323\",\n    \"X559\", \"X348\", \"X130\", \"X529\", \"X20\", \"X4\", \"X90\", \"X192\", \"X91\", \"X582\", \"X99\",\n    \"X24\", \"X317\", \"X707\", \"X653\", \"X519\", \"X557\", \"X371\", \"X84\", \"X83\", \"X360\",\n    \"X111\", \"X699\", \"X187\", \"X637\", \"X567\", \"X577\", \"X313\", \"X60\", \"X671\", \"X698\",\n    \"X701\", \"X725\", \"X292\", \"X638\", \"X741\", \"X379\", \"X700\", \"X614\", \"X676\", \"X516\",\n    \"X697\", \"X311\", \"X615\", \"X706\", \"X466\", \"X571\", \"X17\", \"X584\", \"X436\", \"X305\",\n    \"X34\", \"X282\", \"X681\", \"X7\", \"X208\", \"X41\", \"X536\", \"X548\", \"X776\", \"X87\", \"X40\",\n    \"X570\", \"X539\", \"X474\", \"X753\", \"X425\", \"X217\", \"X199\", \"X18\", \"X609\", \"X21\",\n    \"X277\", \"X279\", \"X326\", \"X540\", \"X688\", \"X553\", \"X452\", \"X738\", \"X183\",\n    \"label\"\n]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:07:35.692997Z","iopub.execute_input":"2025-07-25T02:07:35.693231Z","iopub.status.idle":"2025-07-25T02:07:35.700623Z","shell.execute_reply.started":"2025-07-25T02:07:35.693207Z","shell.execute_reply":"2025-07-25T02:07:35.699706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = reduce_mem_usage(train, \"train\")\ntest = reduce_mem_usage(test, \"test\")\n\ntrain = train[base_features]\ntest = test[base_features]\n\ntrain = train.dropna().reset_index(drop=True)\ntest = test.fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:07:35.701215Z","iopub.execute_input":"2025-07-25T02:07:35.701491Z","iopub.status.idle":"2025-07-25T02:07:45.836048Z","shell.execute_reply.started":"2025-07-25T02:07:35.701474Z","shell.execute_reply":"2025-07-25T02:07:45.835464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train = add_features(train)\nx_train = add_statistical_features(x_train)\nx_train = add_non_linear_x_market_interactions(x_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:07:45.837362Z","iopub.execute_input":"2025-07-25T02:07:45.837613Z","iopub.status.idle":"2025-07-25T02:08:33.668194Z","shell.execute_reply.started":"2025-07-25T02:07:45.837596Z","shell.execute_reply":"2025-07-25T02:08:33.667315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test = add_features(test)\nx_test = add_statistical_features(x_test)\nx_test = add_non_linear_x_market_interactions(x_test)\nx_train = x_train[np.isfinite(x_train['label'])].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:08:33.669060Z","iopub.execute_input":"2025-07-25T02:08:33.669277Z","iopub.status.idle":"2025-07-25T02:09:28.856333Z","shell.execute_reply.started":"2025-07-25T02:08:33.669260Z","shell.execute_reply":"2025-07-25T02:09:28.855701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure the index is a datetime type\nx_train.index = pd.to_datetime(x_train.index)\n\n# Sort the dataset by timestamp (index)\nx_train = x_train.sort_index()\n\n# Final training features and labels (no validation split)\nX_train = x_train.drop(columns=['label'])\ny_train = x_train['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:09:28.857219Z","iopub.execute_input":"2025-07-25T02:09:28.857491Z","iopub.status.idle":"2025-07-25T02:09:31.347239Z","shell.execute_reply.started":"2025-07-25T02:09:28.857463Z","shell.execute_reply":"2025-07-25T02:09:31.346688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#feature_names = X_train.columns.tolist()\n#selected_features, importance_df = select_top_k_features_by_shap(X_train, y_train, feature_names, k=95)\n\nselected_features = ['X451', 'X758', 'X452', 'X345', 'X752', 'X425', 'X466', 'X757', 'X178', 'X137', 'X759', 'X174', 'X777', 'X611', 'X752_log_x_buy_qty', 'X303', 'X128', 'X40', 'X757_log_x_volume', 'X282', 'X280', 'X385', 'X759_log_x_x_stat_median', 'X780', 'X752_log_x_volume', 'X20', 'X584', 'X292', 'X501', 'x_stat_p75', 'X612', 'X752_log_x_sell_qty', 'X415_log_x_buy_qty', 'X24', 'X326', 'X738', 'X421', 'X178_log_x_volume', 'X759_log_x_sell_qty', 'X296', 'X287', 'X427', 'X21', 'X759_log_x_buy_qty', 'X586', 'X570', 'X507', 'X91', 'X591', 'X35', 'X37', 'X302', 'X753_log_x_buy_qty', 'X614_log_x_volume', 'X452_log_x_volume', 'X345_log_x_buy_qty', 'X298', 'X413', 'X576', 'X415', 'X415_log_x_volume', 'X757_log_x_sell_qty', 'X37_log_x_sell_qty', 'X466_log_x_volume', 'X653', 'X614', 'X602', 'X425_log_x_volume', 'X757_log_x_buy_qty', 'X87', 'X425_log_x_buy_qty', 'X130', 'X282_log_x_volume', 'X18', 'X582', 'X753_log_x_sell_qty', 'X287_log_x_sell_qty', 'X577', 'X256', 'X776', 'X288', 'X137_log_x_volume', 'X21_log_x_x_stat_median', 'X279', 'X385_log_x_volume', 'X752_log_x_x_stat_mean', 'X178_log_x_buy_qty', 'X84', 'X674', 'X611_log_x_volume', 'X299', 'X345_log_x_x_stat_mean', 'X529', 'X277_log_x_buy_qty', 'X90', 'order_flow_imbalance', 'kyle_lambda', 'bid_ask_spread', 'liquidity_imbalance']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:09:39.211199Z","iopub.execute_input":"2025-07-25T02:09:39.211476Z","iopub.status.idle":"2025-07-25T02:09:39.216660Z","shell.execute_reply.started":"2025-07-25T02:09:39.211455Z","shell.execute_reply":"2025-07-25T02:09:39.215995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train[selected_features]\nX_test = x_test[selected_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:09:39.673698Z","iopub.execute_input":"2025-07-25T02:09:39.674287Z","iopub.status.idle":"2025-07-25T02:09:40.036571Z","shell.execute_reply.started":"2025-07-25T02:09:39.674270Z","shell.execute_reply":"2025-07-25T02:09:40.036054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:09:43.892698Z","iopub.execute_input":"2025-07-25T02:09:43.893170Z","iopub.status.idle":"2025-07-25T02:09:43.950930Z","shell.execute_reply.started":"2025-07-25T02:09:43.893151Z","shell.execute_reply":"2025-07-25T02:09:43.950344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef simple_prophet_outlier_detection(df, feature_col, timestamp_col='__index_level_0__'):\n    \"\"\"\n    Simple example of using Prophet to detect outliers in a single feature.\n    \n    The idea: Prophet models the expected behavior of the feature over time.\n    Points that fall far outside Prophet's confidence interval are outliers.\n    \"\"\"\n    print(f\"Detecting outliers in {feature_col} using Prophet...\")\n    \n    # 1. Prepare data for Prophet\n    prophet_df = pd.DataFrame({\n        'ds': df[timestamp_col],\n        'y': df[feature_col]\n    })\n    \n    # Remove obvious bad values\n    prophet_df = prophet_df[np.isfinite(prophet_df['y'])]\n    \n    # Resample to hourly for efficiency (adjust based on your needs)\n    prophet_hourly = prophet_df.set_index('ds').resample('1H').mean().reset_index()\n    prophet_hourly = prophet_hourly.dropna()\n    \n    # 2. Fit Prophet model\n    model = Prophet(\n        changepoint_prior_scale=0.05,  # Low value = less sensitive to outliers\n        interval_width=0.95,  # 95% confidence interval\n        yearly_seasonality=False,\n        weekly_seasonality=True,\n        daily_seasonality=True\n    )\n    \n    model.fit(prophet_hourly)\n    \n    # 3. Generate predictions\n    forecast = model.predict(prophet_hourly)\n    \n    # 4. Identify outliers\n    # Method 1: Points outside confidence interval\n    outliers_ci = (\n        (prophet_hourly['y'] < forecast['yhat_lower']) | \n        (prophet_hourly['y'] > forecast['yhat_upper'])\n    )\n    \n    # Method 2: Points with large standardized residuals\n    residuals = prophet_hourly['y'] - forecast['yhat']\n    residual_std = residuals.std()\n    outliers_residual = np.abs(residuals) > 3 * residual_std\n    \n    # Combine both methods\n    outliers = outliers_ci & outliers_residual\n    \n    # 5. Visualize\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # Plot 1: Time series with outliers\n    ax1.plot(prophet_hourly['ds'], prophet_hourly['y'], 'b.', alpha=0.5, label='Actual')\n    ax1.plot(forecast['ds'], forecast['yhat'], 'g-', linewidth=2, label='Prophet Fit')\n    ax1.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], \n                     alpha=0.2, color='green', label='95% CI')\n    \n    # Highlight outliers\n    outlier_points = prophet_hourly[outliers]\n    ax1.scatter(outlier_points['ds'], outlier_points['y'], \n               color='red', s=100, edgecolor='darkred', linewidth=2, \n               label=f'Outliers ({outliers.sum()})', zorder=10)\n    \n    ax1.set_title(f'Prophet Outlier Detection: {feature_col}')\n    ax1.set_xlabel('Date')\n    ax1.set_ylabel('Value')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Residual distribution\n    ax2.hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')\n    ax2.axvline(x=-3*residual_std, color='red', linestyle='--', label='±3σ threshold')\n    ax2.axvline(x=3*residual_std, color='red', linestyle='--')\n    ax2.set_title('Residual Distribution')\n    ax2.set_xlabel('Residual (Actual - Predicted)')\n    ax2.set_ylabel('Frequency')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 6. Return outlier information\n    outlier_info = {\n        'n_outliers': outliers.sum(),\n        'outlier_pct': outliers.sum() / len(prophet_hourly) * 100,\n        'outlier_timestamps': outlier_points['ds'].tolist(),\n        'outlier_values': outlier_points['y'].tolist(),\n        'residual_std': residual_std\n    }\n    \n    print(f\"Found {outlier_info['n_outliers']} outliers ({outlier_info['outlier_pct']:.2f}%)\")\n    \n    return outlier_info, model, forecast\n\n\n# Quick example for multiple features\ndef detect_outliers_multiple_features(data_path, features_to_check=None):\n    \"\"\"\n    Run outlier detection on multiple features.\n    \"\"\"\n    # # Load data\n    # if features_to_check is None:\n    #     # Default to some common features\n    #     features_to_check = ['volume', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty']\n    \n    # df = pd.read_parquet(data_path, columns=['__index_level_0__'] + features_to_check)\n    \n    # Ensure timestamp\n    if '__index_level_0__' not in df.columns:\n        df['__index_level_0__'] = pd.date_range('2023-03-01', periods=len(df), freq='T')\n    \n    # Analyze each feature\n    outlier_summary = {}\n    \n    for feature in features_to_check:\n        if feature in df.columns:\n            print(f\"\\n{'='*60}\")\n            outlier_info, model, forecast = simple_prophet_outlier_detection(df, feature)\n            outlier_summary[feature] = outlier_info\n    \n    # Summary plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    features = list(outlier_summary.keys())\n    outlier_pcts = [outlier_summary[f]['outlier_pct'] for f in features]\n    \n    bars = ax.bar(features, outlier_pcts, color='coral', edgecolor='darkred', linewidth=2)\n    \n    # Highlight high outlier features\n    for i, pct in enumerate(outlier_pcts):\n        if pct > 1.0:  # More than 1% outliers\n            bars[i].set_color('red')\n    \n    ax.set_title('Outlier Percentage by Feature', fontsize=14, fontweight='bold')\n    ax.set_ylabel('Outlier %')\n    ax.set_xlabel('Feature')\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels\n    for bar, pct in zip(bars, outlier_pcts):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{pct:.2f}%', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return outlier_summary\n\n\n# Practical example: Using outlier detection for data cleaning\ndef clean_feature_outliers(df, feature_col, outlier_info, method='cap'):\n    \"\"\"\n    Clean outliers from a feature using different methods.\n    \"\"\"\n    # Get outlier timestamps\n    outlier_timestamps = outlier_info['outlier_timestamps']\n    \n    # Create copy\n    df_clean = df.copy()\n    \n    if method == 'remove':\n        # Remove rows with outliers\n        mask = ~df_clean['__index_level_0__'].isin(outlier_timestamps)\n        df_clean = df_clean[mask]\n        print(f\"Removed {len(outlier_timestamps)} rows with outliers\")\n        \n    elif method == 'cap':\n        # Cap outliers at 99th percentile\n        lower_cap = df_clean[feature_col].quantile(0.01)\n        upper_cap = df_clean[feature_col].quantile(0.99)\n        \n        df_clean[feature_col] = df_clean[feature_col].clip(lower=lower_cap, upper=upper_cap)\n        print(f\"Capped {feature_col} to range [{lower_cap:.2f}, {upper_cap:.2f}]\")\n        \n    elif method == 'interpolate':\n        # Replace outliers with interpolated values\n        outlier_mask = df_clean['__index_level_0__'].isin(outlier_timestamps)\n        df_clean.loc[outlier_mask, feature_col] = np.nan\n        df_clean[feature_col] = df_clean[feature_col].interpolate(method='linear')\n        print(f\"Interpolated {len(outlier_timestamps)} outlier values\")\n    \n    return df_clean\n\n\n# Define your feature list\n\nX_FEATURES = selected_features\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1: Single feature analysis\n    data_path = '/kaggle/input/drw-crypto-market-prediction/train.parquet'\n    df = pd.read_parquet(data_path, columns=['__index_level_0__', 'volume', 'label'])\n    X_train_reset = X_train.reset_index(drop=True)\n#    X_train_reset.index = df.index[:len(X_train)]  # Match df's index\n    df = pd.concat([df,X_train_reset])\n    if '__index_level_0__' not in df.columns:\n        df['__index_level_0__'] = pd.date_range('2023-03-01', periods=len(df), freq='T')\n    print(X_train_reset)\n    # Detect outliers in volume\n    outlier_info, model, forecast = simple_prophet_outlier_detection(df, 'volume')\n\n    print(X_FEATURES)\n    # Example 2: Multiple features\n    outlier_summary = detect_outliers_multiple_features(\n        data_path,\n        X_FEATURES\n    )\n    \n    # Example 3: Clean the data\n    df_clean = df.copy()\n    for feat in X_FEATURES:\n        if feat in outlier_summary:\n            df_clean = clean_feature_outliers(df_clean, feat, outlier_summary[feat], method='cap')\n    print(\"\\nOutlier detection complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:10:00.009095Z","iopub.execute_input":"2025-07-25T02:10:00.009667Z","iopub.status.idle":"2025-07-25T02:15:17.898008Z","shell.execute_reply.started":"2025-07-25T02:10:00.009644Z","shell.execute_reply":"2025-07-25T02:15:17.897201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install koolbox scikit-learn==1.5.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:16:21.917055Z","iopub.execute_input":"2025-07-25T02:16:21.917569Z","iopub.status.idle":"2025-07-25T02:16:29.582030Z","shell.execute_reply.started":"2025-07-25T02:16:21.917543Z","shell.execute_reply":"2025-07-25T02:16:29.581249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from koolbox import Trainer\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom scipy.stats import pearsonr as pr\nfrom xgboost import XGBRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:16:33.832747Z","iopub.execute_input":"2025-07-25T02:16:33.833013Z","iopub.status.idle":"2025-07-25T02:16:37.432540Z","shell.execute_reply.started":"2025-07-25T02:16:33.832992Z","shell.execute_reply":"2025-07-25T02:16:37.431936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pearsonr(y_true, y_pred):\n    return pr(y_true, y_pred)[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:16:37.433738Z","iopub.execute_input":"2025-07-25T02:16:37.434347Z","iopub.status.idle":"2025-07-25T02:16:37.438137Z","shell.execute_reply.started":"2025-07-25T02:16:37.434328Z","shell.execute_reply":"2025-07-25T02:16:37.437363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_params = {\n    \"boosting_type\": \"gbdt\",\n    \"colsample_bytree\": 0.5625888953382505,\n    \"learning_rate\": 0.029312951475451557,\n    \"min_child_samples\": 63,\n    \"min_child_weight\": 0.11456572852335424,\n    \"n_estimators\": 126,\n    \"n_jobs\": -1,\n    \"num_leaves\": 37,\n    \"random_state\": 42,\n    \"reg_alpha\": 85.2476527854083,\n    \"reg_lambda\": 99.38305361388907,\n    \"subsample\": 0.450669817684892,\n    \"verbose\": -1\n}\n\nlgbm_goss_params = {\n    \"boosting_type\": \"goss\",\n    \"colsample_bytree\": 0.34695458228489784,\n    \"learning_rate\": 0.031023014900595287,\n    \"min_child_samples\": 30,\n    \"min_child_weight\": 0.4727729225033618,\n    \"n_estimators\": 220,\n    \"n_jobs\": -1,\n    \"num_leaves\": 58,\n    \"random_state\": 42,\n    \"reg_alpha\": 38.665994901468224,\n    \"reg_lambda\": 92.76991677464294,\n    \"subsample\": 0.4810891284493255,\n    \"verbose\": -1\n}\n\nxgb_params = {\n    \"colsample_bylevel\": 0.4778015829774066,\n    \"colsample_bynode\": 0.362764358742407,\n    \"colsample_bytree\": 0.7107423488010493,\n    \"gamma\": 1.7094857725240398,\n    \"learning_rate\": 0.02213323588455387,\n    \"max_depth\": 20,\n    \"max_leaves\": 12,\n    \"min_child_weight\": 16,\n    \"n_estimators\": 1667,\n    \"n_jobs\": -1,\n    \"random_state\": 42,\n    \"reg_alpha\": 39.352415706891264,\n    \"reg_lambda\": 75.44843704068275,\n    \"subsample\": 0.06566669853471274,\n    \"verbosity\": 0\n}\n\nhistgb_params = {\n    \"max_iter\": 500,\n    \"learning_rate\": 0.05,\n    \"max_depth\": 10,\n    \"random_state\": 42\n}\n\ncatboost_params = {\n    \"iterations\": 1000,\n    \"learning_rate\": 0.02,\n    \"depth\": 10,\n    \"l2_leaf_reg\": 3,\n    \"bootstrap_type\": \"Bayesian\",\n    \"bagging_temperature\": 1.0,\n    \"loss_function\": \"RMSE\",\n    \"eval_metric\": \"RMSE\",\n    \"early_stopping_rounds\": 100,\n    \"random_seed\": 42,\n    \"task_type\": \"GPU\",\n    \"verbose\": 100\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:16:37.439034Z","iopub.execute_input":"2025-07-25T02:16:37.439223Z","iopub.status.idle":"2025-07-25T02:16:37.472279Z","shell.execute_reply.started":"2025-07-25T02:16:37.439210Z","shell.execute_reply":"2025-07-25T02:16:37.471744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fold_scores = {}\noverall_scores = {}\n\noof_preds = {}\ntest_preds = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:19:59.627124Z","iopub.execute_input":"2025-07-25T03:19:59.627373Z","iopub.status.idle":"2025-07-25T03:19:59.631349Z","shell.execute_reply.started":"2025-07-25T03:19:59.627356Z","shell.execute_reply":"2025-07-25T03:19:59.630565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    train_path = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n    test_path = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n    sample_sub_path = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n\n    target = \"label\"\n    n_folds = 5\n    seed = 42\n\n    run_optuna = True\n    n_optuna_trials = 500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:16:39.733018Z","iopub.execute_input":"2025-07-25T02:16:39.733689Z","iopub.status.idle":"2025-07-25T02:16:39.737467Z","shell.execute_reply.started":"2025-07-25T02:16:39.733659Z","shell.execute_reply":"2025-07-25T02:16:39.736870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = df_clean\n# For each column, replace NaN with median for robustness\nfor col in df_clean.columns:\n    if df_clean[col].isna().any():\n        median_val = df_clean[col].median()\n        df_clean[col] = df_clean[col].fillna( 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:51.800967Z","iopub.execute_input":"2025-07-25T02:51:51.801215Z","iopub.status.idle":"2025-07-25T02:51:52.107411Z","shell.execute_reply.started":"2025-07-25T02:51:51.801198Z","shell.execute_reply":"2025-07-25T02:51:52.106959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop(CFG.target, axis=1)\ny = train[CFG.target]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:52.108717Z","iopub.execute_input":"2025-07-25T02:51:52.109016Z","iopub.status.idle":"2025-07-25T02:51:52.385554Z","shell.execute_reply.started":"2025-07-25T02:51:52.108989Z","shell.execute_reply":"2025-07-25T02:51:52.384768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = X.drop(columns=[\"volume\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:52.453326Z","iopub.execute_input":"2025-07-25T02:51:52.453587Z","iopub.status.idle":"2025-07-25T02:51:52.851455Z","shell.execute_reply.started":"2025-07-25T02:51:52.453567Z","shell.execute_reply":"2025-07-25T02:51:52.850871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=X.reset_index(drop=True) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:52.852449Z","iopub.execute_input":"2025-07-25T02:51:52.852679Z","iopub.status.idle":"2025-07-25T02:51:53.043998Z","shell.execute_reply.started":"2025-07-25T02:51:52.852662Z","shell.execute_reply":"2025-07-25T02:51:53.043405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=X.drop(columns=['__index_level_0__'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:53.214099Z","iopub.execute_input":"2025-07-25T02:51:53.214378Z","iopub.status.idle":"2025-07-25T02:51:53.590309Z","shell.execute_reply.started":"2025-07-25T02:51:53.214355Z","shell.execute_reply":"2025-07-25T02:51:53.589761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X= X[525886:]\ny = y[:525886]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T02:51:53.615413Z","iopub.execute_input":"2025-07-25T02:51:53.615648Z","iopub.status.idle":"2025-07-25T02:51:53.619224Z","shell.execute_reply.started":"2025-07-25T02:51:53.615632Z","shell.execute_reply":"2025-07-25T02:51:53.618582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM (gbdt)","metadata":{}},{"cell_type":"code","source":"lgbm_trainer = Trainer(\n    LGBMRegressor(**lgbm_params),\n    cv=KFold(n_splits=5, shuffle=False),\n    metric=pearsonr,\n    task=\"regression\",\n    metric_precision=6\n)\n\nlgbm_trainer.fit(X,y)\n\nfold_scores[\"LightGBM (gbdt)\"] = lgbm_trainer.fold_scores\noverall_scores[\"LightGBM (gbdt)\"] = [pearsonr(lgbm_trainer.oof_preds, y)]\noof_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.oof_preds\ntest_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:20:06.780629Z","iopub.execute_input":"2025-07-25T03:20:06.781195Z","iopub.status.idle":"2025-07-25T03:20:25.285135Z","shell.execute_reply.started":"2025-07-25T03:20:06.781173Z","shell.execute_reply":"2025-07-25T03:20:25.284507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM (goss) ","metadata":{}},{"cell_type":"code","source":"lgbm_goss_trainer = Trainer(\n    LGBMRegressor(**lgbm_goss_params),\n    cv=KFold(n_splits=5, shuffle=False),\n    metric=pearsonr,\n    task=\"regression\",\n    metric_precision=6\n)\n\nlgbm_goss_trainer.fit(X,y)\n\nfold_scores[\"LightGBM (goss)\"] = lgbm_goss_trainer.fold_scores\noverall_scores[\"LightGBM (goss)\"] = [pearsonr(lgbm_goss_trainer.oof_preds, y)]\noof_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.oof_preds\ntest_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:20:25.286211Z","iopub.execute_input":"2025-07-25T03:20:25.286845Z","iopub.status.idle":"2025-07-25T03:21:16.524850Z","shell.execute_reply.started":"2025-07-25T03:20:25.286820Z","shell.execute_reply":"2025-07-25T03:21:16.524202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"xgb_trainer = Trainer(\n    XGBRegressor(**xgb_params),\n    cv=KFold(n_splits=5, shuffle=False),\n    metric=pearsonr,\n    task=\"regression\",\n    metric_precision=6\n)\n\nxgb_trainer.fit(X,y)\n\nfold_scores[\"XGBoost\"] = xgb_trainer.fold_scores\noverall_scores[\"XGBoost\"] = [pearsonr(xgb_trainer.oof_preds, y)]\noof_preds[\"XGBoost\"] = xgb_trainer.oof_preds\ntest_preds[\"XGBoost\"] = xgb_trainer.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:21:16.525405Z","iopub.execute_input":"2025-07-25T03:21:16.525621Z","iopub.status.idle":"2025-07-25T03:22:15.365333Z","shell.execute_reply.started":"2025-07-25T03:21:16.525604Z","shell.execute_reply":"2025-07-25T03:22:15.364706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingRegressor\n\nhistgb_params = {\n    \"max_iter\": 500,\n    \"learning_rate\": 0.05,\n    \"max_depth\": 10,\n    \"random_state\": 42\n}\n\nhistgb_trainer = Trainer(\n    HistGradientBoostingRegressor(**histgb_params),\n    cv=KFold(n_splits=5, shuffle=False),\n    metric=pearsonr,\n    task=\"regression\",\n    metric_precision=6\n)\n\nhistgb_trainer.fit(X, y)\nfold_scores[\"HistGB\"] = histgb_trainer.fold_scores\noverall_scores[\"HistGB\"] = [pearsonr(histgb_trainer.oof_preds, y)]\noof_preds[\"HistGB\"] = histgb_trainer.oof_preds\ntest_preds[\"HistGB\"] = histgb_trainer.predict(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:22:15.366844Z","iopub.execute_input":"2025-07-25T03:22:15.367213Z","iopub.status.idle":"2025-07-25T03:23:00.594929Z","shell.execute_reply.started":"2025-07-25T03:22:15.367187Z","shell.execute_reply":"2025-07-25T03:23:00.594248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_weights(weights, title):\n    sorted_indices = np.argsort(weights[0])[::-1]\n    sorted_coeffs = np.array(weights[0])[sorted_indices]\n    sorted_model_names = np.array(list(oof_preds.keys()))[sorted_indices]\n\n    plt.figure(figsize=(10, weights.shape[1] * 0.5))\n    ax = sns.barplot(x=sorted_coeffs, y=sorted_model_names, palette=\"RdYlGn_r\")\n\n    for i, (value, name) in enumerate(zip(sorted_coeffs, sorted_model_names)):\n        if value >= 0:\n            ax.text(value, i, f\"{value:.3f}\", va=\"center\", ha=\"left\", color=\"black\")\n        else:\n            ax.text(value, i, f\"{value:.3f}\", va=\"center\", ha=\"right\", color=\"black\")\n\n    xlim = ax.get_xlim()\n    ax.set_xlim(xlim[0] - 0.1 * abs(xlim[0]), xlim[1] + 0.1 * abs(xlim[1]))\n\n    plt.title(title)\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:23:45.846318Z","iopub.execute_input":"2025-07-25T03:23:45.846634Z","iopub.status.idle":"2025-07-25T03:23:45.853459Z","shell.execute_reply.started":"2025-07-25T03:23:45.846614Z","shell.execute_reply":"2025-07-25T03:23:45.852779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = pd.DataFrame(oof_preds)\nX_test = pd.DataFrame(test_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:23:46.175965Z","iopub.execute_input":"2025-07-25T03:23:46.176243Z","iopub.status.idle":"2025-07-25T03:23:46.187739Z","shell.execute_reply.started":"2025-07-25T03:23:46.176222Z","shell.execute_reply":"2025-07-25T03:23:46.187085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(X, \"oof_preds.pkl\")\njoblib.dump(X_test, \"test_preds.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:23:46.514337Z","iopub.execute_input":"2025-07-25T03:23:46.514639Z","iopub.status.idle":"2025-07-25T03:23:46.594460Z","shell.execute_reply.started":"2025-07-25T03:23:46.514617Z","shell.execute_reply":"2025-07-25T03:23:46.593845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\nimport tensorflow as tf\nimport numpy as np\n\nclass AutoEncoderMLP(BaseEstimator, RegressorMixin):\n    def __init__(self, num_columns, hidden_units, dropout_rates, lr=1e-3):\n        self.num_columns = num_columns\n        self.hidden_units = hidden_units\n        self.dropout_rates = dropout_rates\n        self.lr = lr\n        self.model = self._build_model()\n    \n    def _build_model(self):\n        inp = tf.keras.layers.Input(shape=(self.num_columns,))\n        x0 = tf.keras.layers.BatchNormalization()(inp)\n\n        encoder = tf.keras.layers.GaussianNoise(self.dropout_rates[0])(x0)\n        encoder = tf.keras.layers.Dense(self.hidden_units[0])(encoder)\n        encoder = tf.keras.layers.BatchNormalization()(encoder)\n        encoder = tf.keras.layers.Activation('swish')(encoder)\n\n        decoder = tf.keras.layers.Dropout(self.dropout_rates[1])(encoder)\n        decoder = tf.keras.layers.Dense(self.num_columns, name='decoder')(decoder)\n\n        x_reg = tf.keras.layers.Dense(self.hidden_units[1])(encoder)\n        x_reg = tf.keras.layers.BatchNormalization()(x_reg)\n        x_reg = tf.keras.layers.Activation('swish')(x_reg)\n        x_reg = tf.keras.layers.Dropout(self.dropout_rates[2])(x_reg)\n\n        out_reg = tf.keras.layers.Dense(1, activation='linear', name='target')(x_reg)\n\n        model = tf.keras.models.Model(inputs=inp, outputs=[decoder, out_reg])\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr),\n            loss={\"decoder\": tf.keras.losses.MeanSquaredError(),\n                  \"target\": tf.keras.losses.MeanSquaredError()},\n            loss_weights={\"decoder\": 0.3, \"target\": 1.0}\n        )\n        return model\n\n    def fit(self, X, y):\n        self.model.fit(\n            X, {\"decoder\": X, \"target\": y},\n            epochs=50,\n            batch_size=8192,\n            validation_split=0.2,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n                tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n            ],\n            verbose=0\n        )\n        return self\n\n    def predict(self, X):\n        _, y_pred = self.model.predict(X, verbose=0)\n        return y_pred.flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:23:50.080259Z","iopub.execute_input":"2025-07-25T03:23:50.080563Z","iopub.status.idle":"2025-07-25T03:23:50.090259Z","shell.execute_reply.started":"2025-07-25T03:23:50.080513Z","shell.execute_reply":"2025-07-25T03:23:50.089393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ae_model = AutoEncoderMLP(\n    num_columns=X.shape[1],\n    hidden_units=[128, 128],\n    dropout_rates=[0.05, 0.1, 0.2],\n    lr=1e-3\n)\n\n\nae_trainer = Trainer(\n    ae_model,\n    cv=KFold(n_splits=5, shuffle=False),\n    metric=pearsonr,\n    task=\"regression\",\n    metric_precision=6\n)\n\nae_trainer.fit(X, y)\n\nfold_scores[\"ae\"] = ae_trainer.fold_scores\noverall_scores[\"ae\"] = [pearsonr(ae_trainer.oof_preds, y)]\noof_preds[\"ae\"] = ae_trainer.oof_preds\ntest_preds[\"ae\"] = ae_trainer.predict(X_test)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:24:37.242579Z","iopub.execute_input":"2025-07-25T03:24:37.243201Z","iopub.status.idle":"2025-07-25T03:29:15.345278Z","shell.execute_reply.started":"2025-07-25T03:24:37.243176Z","shell.execute_reply":"2025-07-25T03:29:15.344680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds[\"XGBoost\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:34:59.925020Z","iopub.execute_input":"2025-07-25T03:34:59.925295Z","iopub.status.idle":"2025-07-25T03:34:59.930789Z","shell.execute_reply.started":"2025-07-25T03:34:59.925276Z","shell.execute_reply":"2025-07-25T03:34:59.930084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(CFG.sample_sub_path)\nsub[\"prediction\"] = test_preds[\"XGBoost\"]\nsub.to_csv(f\"sub_xg.csv\", index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:37:51.145554Z","iopub.execute_input":"2025-07-25T03:37:51.146073Z","iopub.status.idle":"2025-07-25T03:37:52.296813Z","shell.execute_reply.started":"2025-07-25T03:37:51.146050Z","shell.execute_reply":"2025-07-25T03:37:52.296108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"overall_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:36:36.145379Z","iopub.execute_input":"2025-07-25T03:36:36.145644Z","iopub.status.idle":"2025-07-25T03:36:36.152679Z","shell.execute_reply.started":"2025-07-25T03:36:36.145627Z","shell.execute_reply":"2025-07-25T03:36:36.152090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = pd.DataFrame(fold_scores)\nmean_scores = scores.mean().sort_values(ascending=False)\norder = scores.mean().sort_values(ascending=False).index.tolist()\n\nmin_score = mean_scores.min()\nmax_score = mean_scores.max()\npadding = (max_score - min_score) * 0.5\nlower_limit = min_score - padding\nupper_limit = max_score + padding\n\nfig, axs = plt.subplots(1, 2, figsize=(15, scores.shape[1] * 0.5))\n\nboxplot = sns.boxplot(data=scores, order=order, ax=axs[0], orient=\"h\", color=\"grey\")\naxs[0].set_title(f\"Fold Score\")\naxs[0].set_xlabel(\"\")\naxs[0].set_ylabel(\"\")\n\nbarplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], color=\"grey\")\naxs[1].set_title(f\"Average Score\")\naxs[1].set_xlabel(\"\")\naxs[1].set_xlim(left=lower_limit, right=upper_limit)\naxs[1].set_ylabel(\"\")\n\nfor i, (score, model) in enumerate(zip(mean_scores.values, mean_scores.index)):\n    color = \"cyan\" if \"ensemble\" in model.lower() else \"grey\"\n    barplot.patches[i].set_facecolor(color)\n    boxplot.patches[i].set_facecolor(color)\n    barplot.text(score, i, round(score, 6), va=\"center\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:30:48.276922Z","iopub.execute_input":"2025-07-25T03:30:48.277492Z","iopub.status.idle":"2025-07-25T03:30:48.561043Z","shell.execute_reply.started":"2025-07-25T03:30:48.277468Z","shell.execute_reply":"2025-07-25T03:30:48.560255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T00:55:48.660814Z","iopub.execute_input":"2025-07-25T00:55:48.661089Z","iopub.status.idle":"2025-07-25T00:55:48.671079Z","shell.execute_reply.started":"2025-07-25T00:55:48.661068Z","shell.execute_reply":"2025-07-25T00:55:48.670298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}